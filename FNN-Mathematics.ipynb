{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Networks\n",
    "![FNN](https://upload.wikimedia.org/wikipedia/commons/0/00/Multi-Layer_Neural_Network-Vector-Blank.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An artificial neural network (ANN) is a machine learning model inspired by the biological brain. It consists of interconnected nodes that performs an operation on a piece of data that can be tuned to output something specific. You can think of the individual nodes of an ANN as the neurons in a brain and the connections between them as the synapses. Training the ANN means that we increase or decrease the strength of different connections until we achieve some desired behavior. \n",
    "\n",
    "The most common type of ANN is a feedforward neural network (FNN). It consists of nodes that are organized into layers: the input layer, the hidden layer(s), and the output layer. The nodes within a given layer have no connections between them, as seen in the picture above. Each layer can only pass information to the subsequent layer only, which is why FNNs has 'feedforward' in the name.\n",
    "\n",
    "Feedforward neural networks are powerful tools that can be applied to supervised, unsupervised, and reinforcement learning problems. They are especially useful when dealing with large amounts of nonlinear data. However, they can be computationally heavy compared to many of the algorithms we have discussed so far, so they [shouldn't be used](https://medium.datadriveninvestor.com/when-not-to-use-neural-networks-89fb50622429) if a simpler algorithm does the job. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Foundation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation\n",
    "\n",
    "For demonstration purposes, let's assume our network has only **one hidden layer** and only **one output**. This will help make the math a little easier to write down, but the general idea still holds for deeper networks and for networks with multiple outputs. You would just have to change the notation a bit and carry out the mathematics as shown below. \n",
    "\n",
    "Each layer of nodes can be represented as vectors: the input layer $\\vec{x}$, the hidden layer $\\vec{h}$, and the output layer $\\vec{y}$. Since we've assumed that the output layer has only one node, we'll just write it as a scalar $y$. The connections between the input layer $\\vec{x}$ and the hidden layer $\\vec{h}$ can be parameterized by a weight matrix $W$ and a bias vector $\\vec{b}$:\n",
    "\n",
    "$$\\vec{h} = \\vec{f}(W\\vec{x}+\\vec{b}).$$\n",
    "\n",
    "This transformation consists of performing an [affine](https://eli.thegreenplace.net/2018/affine-transformations/) transformation, then passing the result into an activation function $f(x)$. It is **very important** that $f(x)$ is chosen to be a nonlinear function that is easy to calculate (more on this later). The notation $\\vec{f}(\\vec{x})$ is meant to emphasize that each element of the vector $\\vec{x}$ is passed into the activation function, resulting in another vector. In a similar way, the connections between the hidden layer $\\vec{h}$ and the single output node $y$ can be parametrized by another set of weights $\\vec{w}$ and a bias $c$.\n",
    "\n",
    "$$y = g(\\vec{w}^T \\vec{h}+c).$$\n",
    "\n",
    "Notice that the activation function here is labeled by a different name $g(x)$. This is because the activation function for the output layer is usually different than the one for the hidden layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "\n",
    "[Here are the available activation functions in Keras.](https://keras.io/api/layers/activations/)\n",
    "\n",
    "Activation functions are loosely inspired by the firing of neurons in reponse to a signal. There's normally some minimum threshold potential (or activation potential) required for a neuron to transmit any signal. Then as the input signal increases, the transmitted signal also increases until it reaches a maximum. \n",
    "\n",
    "A common choice for the hidden layer activation function $f(x)$ is the Rectified Linear Unit (ReLU). However if the weights become large and negative, it can cause ReLU to return 0 identically. Then, smoother approximations of ReLU, such as SoftPlus or Scaled Exponential Linear Unit (SELU), are preferred. \n",
    "\n",
    "Choosing the right output layer activation function $g(x)$ depends on the task at hand. Many classification problems require bounded activation functions, such as the sigmoid function or hyperbolic tangent. Regression problems require an unbounded activation function and are commonly chosen to be a simple linear function. If the target is known to be positive, you can constrain that by choosing an exponential instead.\n",
    "\n",
    "Here is a plot of some activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-5, 5, 200)\n",
    "\n",
    "sigmoid = 1/(1+np.exp(-x))\n",
    "tanh = np.tanh(x)\n",
    "relu = np.where(x > 0, x, 0)\n",
    "selu = np.where(x > 0, x, np.exp(x)-1)\n",
    "softplus = np.log(1+np.exp(x))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "plt.plot(x, x, label='linear')\n",
    "plt.plot(x, sigmoid, label='sigmoid')\n",
    "plt.plot(x, tanh, label='tanh')\n",
    "plt.plot(x, relu, label='relu')\n",
    "plt.plot(x, selu, label='selu')\n",
    "plt.plot(x, softplus, label='softplus')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Activation Functions')\n",
    "plt.xlim(x[0], x[-1])\n",
    "plt.ylim(-1.5, 3)\n",
    "plt.grid(alpha=0.2)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universal Approximation Theorem\n",
    "\n",
    "Under most conditions, the [universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem) guarantees that a feedforward neural network with one hidden layer (i.e. a shallow FNN) can find an approximate mapping between input data and targets. However, it does not specify how many units will be needed to get a satisfactory mapping. It all depends on the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "When a network is constructed, the weights and biases are initially set to some random values near, but not exactly, zero. During the training process, the parameters change bit by bit using a method called **gradient descent**. The cost function $C(y, \\hat{y})$ for supervised learning problems depends on the output of the network $y$ and the target $\\hat{y}$. If we let the symbol $\\alpha$ stand in for any of the parameters $W, \\vec{b}, \\vec{w}, c$, then...\n",
    "\n",
    "\n",
    "$$\\min_{\\alpha} C (y, \\hat{y}) \n",
    "\\ \\ \\Longrightarrow \\ \\ \n",
    "\\frac{\\partial C(y, \\hat{y})}{\\partial \\alpha} = \\frac{\\partial C(y, \\hat{y})}{\\partial y} \\frac{\\partial y}{\\partial \\alpha} = 0$$\n",
    "\n",
    "\n",
    "A cost (or loss) function is our way of defining the grading rubric for the training of the network. For regression problems, a safe bet is mean squared error (MSE):\n",
    "\n",
    "$$C(y, \\hat{y})= \\frac{1}{n} \\sum_{i=1}^{n} (y_i-\\hat{y}_i)^2 \n",
    "\\ \\ \\Longrightarrow \\ \\\n",
    "\\frac{\\partial C(y, \\hat{y})}{\\partial \\alpha} = \\frac{2}{n}\\sum_{i=1}^{n} (y_i-\\hat{y}_i) \\frac{\\partial y_i}{\\partial \\alpha}\n",
    "$$\n",
    "\n",
    "For binary classification problems, binary cross-entropy has been shown to outperform MSE:\n",
    "\n",
    "$$C(y, \\hat{y})= -\\frac{1}{n} \\sum_{i=1}^{n} \\hat{y}_i \\log(y_i)+(1-\\hat{y}_i) \\log(1-y_i)\n",
    "\\ \\ \\Longrightarrow \\ \\\n",
    "\\frac{\\partial C(y, \\hat{y})}{\\partial \\alpha} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( -\\frac{\\hat{y}_i}{y_i} + \\frac{1-\\hat{y}_i}{1-y_i} \\right) \\frac{\\partial y_i}{\\partial \\alpha}\n",
    "$$\n",
    "\n",
    "If the data set is very large, taking the sums over a smaller batch of data points will help speed up training. \n",
    "\n",
    "Now, say you've calculated the gradient of the cost function with respect to the parameters $\\frac{\\partial C(y, \\hat{y})}{\\partial \\alpha}$. How do you change the parameters to lower the cost? You can think of this gradient as a vector pointing uphill on the surface of the cost function in parameter space. Going downhill means I want to step in the opposite direction of the gradient. Since this gradient is really just an approximation, I want to reduce my step by some factor called the learning rate $\\eta$:\n",
    "\n",
    "$$\\alpha_{new} = \\alpha_{old} - \\eta*\\frac{\\partial C(y, \\hat{y})}{\\partial \\alpha}$$\n",
    "\n",
    "As training progresses, the learning rate decreases to allow for more fine tuning. The most popular optimization schemes involve an adaptive learning rate and/or \"momentum\" to help get out of local minima. [Adam](https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c) is a great first choice for the optimizer in most applications.\n",
    "\n",
    "\n",
    "To get the explicit formulas for the gradient, it's helpful to write everything in index notation. Assuming we have $M$ hidden nodes and $N$ input nodes...\n",
    "\n",
    "$$y = g(\\vec{w}^T\\vec{h}+c) \\ \\ \\Longrightarrow \\ \\ y = g\\left( \\sum_{i=1}^M w_i h_i + c \\right)$$\n",
    "$$\\vec{h} = \\vec{f}(W\\vec{x}+\\vec{b}) \\ \\ \\Longrightarrow \\ \\ h_i = f_i\\left( \\sum_{j = 1}^N W_{i,j} x_j + b_i \\right)$$\n",
    "\n",
    "We will take derivatives starting from the output layer:\n",
    "\n",
    "$$ \\frac{\\partial y}{\\partial c} = g'(\\vec{w}^T\\vec{h}+c) $$\n",
    "$$ \\frac{\\partial y}{\\partial w_i} = g'(\\vec{w}^T\\vec{h}+c) h_i $$\n",
    "\n",
    "\n",
    "Then using the chain rule, we'll take derivatives for the hidden layer:\n",
    "\n",
    "$$ \\frac{\\partial y}{\\partial b_i} = \\frac{\\partial y}{\\partial h_j} \\frac{\\partial h_j}{\\partial b_i} = g'(\\vec{w}^T\\vec{h}+c) w_j  f'_j\\left( \\sum_{k = 1}^N W_{j,k} x_k + b_j \\right) \\delta_{i,j} =  g'(\\vec{w}^T\\vec{h}+c) w_i  f'_i\\left( \\sum_{j = 1}^N W_{i,j} x_j + b_i \\right)$$\n",
    "$$ \\frac{\\partial y}{\\partial W_{i,j}} = \\frac{\\partial y}{\\partial h_k} \\frac{\\partial h_k}{\\partial W_{i,j}} = g'(\\vec{w}^T\\vec{h}+c) w_k f'_k\\left( \\sum_{l=1}^N W_{k,l} x_l + b_k \\right) \\sum_{l=1}^N \\delta_{i,k} \\delta_{j,l} x_l = g'(\\vec{w}^T\\vec{h}+c) w_i  f'_i\\left( \\sum_{k = 1}^N W_{i,k} x_k + b_i \\right) x_j$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "\n",
    "Putting all this together... here is how you could implement a shallow feedforward neural network. Hopefully this helps make clear what Keras and scikit-learn are doing under the hood. \n",
    "\n",
    "Hidden layer activation function $f(x)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "        \n",
    "    return np.where(x > 0, x, 0)\n",
    "    \n",
    "def relu_deriv(x):\n",
    "        \n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output layer activation functions $g(x)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "        \n",
    "    return 1/(1+np.exp(-x))\n",
    "    \n",
    "def sigmoid_deriv(x):\n",
    "        \n",
    "    return np.exp(-x)/(1+np.exp(-x))**2\n",
    "\n",
    "def linear(x):\n",
    "    \n",
    "    return x\n",
    "\n",
    "def linear_deriv(x):\n",
    "    \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost functions $C(y, \\hat{y})$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_err(y, y_hat):\n",
    "    \n",
    "    return (y-y_hat)**2\n",
    "\n",
    "def squared_err_deriv(y, y_hat):\n",
    "    \n",
    "    return 2*(y-y_hat)\n",
    "\n",
    "def log_loss(y, y_hat):\n",
    "    \n",
    "    return -y_hat*np.log(y) - (1-y_hat)*np.log(1-y)\n",
    "    \n",
    "def log_loss_deriv(y, y_hat):\n",
    "    \n",
    "    return -y_hat/(y+1E-5) + (1-y_hat)/(1-y+1E-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shallow FNN with one output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetwork:\n",
    "    \n",
    "    def __init__(self, n_inputs=1, n_hidden=10, sigma=0.1):\n",
    "\n",
    "        # connects input layer to hidden layer\n",
    "        self.W = np.random.normal(0.0, sigma, (n_hidden, n_inputs))\n",
    "        self.b = np.random.normal(0.0, sigma, n_hidden)\n",
    "        \n",
    "        # connects hidden layer to output node\n",
    "        self.w = np.random.normal(0.0, sigma, n_hidden)\n",
    "        self.c = np.random.normal(0.0, sigma)\n",
    "        \n",
    "        # total number of parameters\n",
    "        self.n_params = n_hidden*(n_inputs+2) + 1\n",
    "        \n",
    "        # convenient indicies to store for splitting gradient \n",
    "        self.indicies = [n_hidden*n_inputs, n_hidden*(n_inputs+1), n_hidden*(n_inputs+2)]\n",
    "\n",
    "    \n",
    "    def forward_pass(self, x):\n",
    "\n",
    "        h = relu(np.dot(self.W, x) + self.b)\n",
    "        y = sigmoid(np.inner(self.w, h) + self.c)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    \n",
    "    def gradient(self, x):\n",
    "        \n",
    "        h = relu(np.dot(self.W, x) + self.b)\n",
    "        df = relu_deriv(np.dot(self.W, x) + self.b)\n",
    "        dg = sigmoid_deriv(np.inner(self.w, h) + self.c)\n",
    "    \n",
    "        # derivative of output w.r.t. output biases\n",
    "        dy_dc = dg\n",
    "        \n",
    "        # derivative of output w.r.t. output weights\n",
    "        dy_dw = dg*h\n",
    "        \n",
    "        # derivative of output w.r.t. hidden nodes (for chain rule)\n",
    "        dy_dh = dg*self.w\n",
    "        \n",
    "        # derivative of output w.r.t. hidden biases\n",
    "        dy_db = np.multiply(dy_dh, df)\n",
    "        \n",
    "        # derivative of output w.r.t. hidden weights\n",
    "        dy_dW = np.outer(np.multiply(dy_dh, df), x)\n",
    "    \n",
    "        return np.concatenate((dy_dW, dy_db, dy_dw, dy_dc), axis=None)\n",
    "    \n",
    "    \n",
    "    def update_params(self, gradient, learning_rate):\n",
    "        \n",
    "        # split gradient into subarrays\n",
    "        dW, db, dw, dc = np.split(gradient, self.indicies)\n",
    "        \n",
    "        self.W -= learning_rate*dW.reshape(self.W.shape)\n",
    "        self.b -= learning_rate*db\n",
    "        self.w -= learning_rate*dw\n",
    "        self.c -= learning_rate*dc[0]\n",
    "    \n",
    "    \n",
    "    def fit(self, X_train, y_train, n_epochs=1000, learning_rate=0.001, decay=0.01):\n",
    "        \n",
    "        # total number of training data points\n",
    "        n_train = X_train.shape[0]\n",
    "        \n",
    "        print(\"{:>10s} {:>10s} {:>10s}\".format(\"Epoch\", \"Cost\", \"||Grad||\"))\n",
    "        \n",
    "        # for storing cost and ||grad|| during training\n",
    "        training = np.empty((n_epochs+1, 2))\n",
    "        \n",
    "        # loop over training epochs\n",
    "        for epoch in range(n_epochs+1):\n",
    "            \n",
    "            cost = 0.0\n",
    "            grad = np.zeros(self.n_params)\n",
    "\n",
    "            for i in range(n_train):\n",
    "\n",
    "                # output of network\n",
    "                y = self.forward_pass(X_train[i])\n",
    "\n",
    "                # calculate cost\n",
    "                cost += log_loss(y, y_train[i])\n",
    "\n",
    "                # calculate gradient of cost w.r.t. parameters\n",
    "                grad += log_loss_deriv(y, y_train[i])*FNN.gradient(X_train[i])\n",
    "\n",
    "\n",
    "            # take average\n",
    "            cost /= n_train\n",
    "            grad /= n_train\n",
    "            \n",
    "            # store\n",
    "            training[epoch, 0] = cost\n",
    "            training[epoch, 1] = np.linalg.norm(grad)\n",
    "            \n",
    "            # print updates\n",
    "            if epoch%100 == 0:\n",
    "                print(\"{:10d} {:10f} {:10f}\".format(epoch, cost, np.linalg.norm(grad)))\n",
    "            \n",
    "            # gradient descent with decaying learning rate\n",
    "            self.update_params(grad, learning_rate/(1+decay*epoch))\n",
    "        \n",
    "        return pd.DataFrame(training, columns=['Cost', 'Gradient'])\n",
    "            \n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \n",
    "        n_test = X_test.shape[0]\n",
    "        y_pred = np.empty(n_test)\n",
    "        \n",
    "        for i in range(n_test):\n",
    "            y_pred[i] = self.forward_pass(X_test[i])\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is just for your reference. We will be using Keras to build our networks to take advantage of all the different options it provides."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
